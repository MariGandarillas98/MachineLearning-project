---
title: "Machine Learning Project"
format: html
editor: visual
self-contained: true
toc: true
toc-depth: 3
toc-location: left
---

```{r setup, include=FALSE}
install.packages("here")
install.packages("ROCR")
install.packages("DT")
install.packages("gt")
install.packages("expss")
install.packages("cowplot")
install.packages("vcd")
install.packages("grid")
install.packages(c("leaflet","leaflet.extras", "dplyr"))
install.packages("sf", dependencies=TRUE, repos='http://cran.rstudio.com/')
install.packages("leaflet")
install.packages("kableExtra")
install.packages("Metrics")
install.packages("mlr")
install.packages("caret")
install.packages("ISLR")
install.packages("kernlab")
install.packages("htmltools")
library(dplyr)
library(haven)
library(tidyr)
library(reshape2)
library(readxl)
library(here)
library(tidyverse)
library(knitr)
library(corrplot)
library(lmtest)  
library("pROC")  
library(ROCR)  
library(lubridate)  
library(pscl)  
library(carData)  
library(car)
library(pROC)
library(ROCR)
library(ggplot2)
library(ggpubr)
library(ggridges)
library(cowplot)
library(vcd)
library(sf)
library(kableExtra)
library(gridExtra)
library(visdat)
library(liver)
library(rpart)
library(rpart.plot)
library(ranger)
library(summarytools)
library(ISLR)
library(e1071)
library(kernlab)
library(caret)
library(Metrics)
library(mlr)
```

# Poverty Analysis in The United States 2024

**Machine Learning in Business Analytics - Spring 2024**

**Marcela Choque Quispe, Reisa Reci, and Mariel Gandarillas Calderon**

**2024-06-09**

*This project was written by us and in our own words, except for quotations from published and unpublished sources, which are clearly indicated and acknowledged as such. We are conscious that the incorporation of material from other works or a paraphrase of such material without acknowledgement will be treated as plagiarism, subject to the custom and usage of the subject, according to the University Regulations. The source of any picture, map or other illustration is also indicated, as is the source, published or unpublished, of any material not resulting from our own research.*

## 1. Abstract

This research project delves into the socioeconomic dimensions of poverty in the United States, leveraging the comprehensive dataset provided by the Household Survey USA 2024. The investigation centers on a meticulous examination of various demographic and socioeconomic variables, including race, age, gender, geographic location, educational attainment, marital status, and income, to delineate the contours of poverty across the nation. By aligning the income metrics with the poverty threshold defined by the United Nations, this study categorizes individuals into "poor" and "non-poor" groups, facilitating a nuanced analysis of poverty's underpinnings.

Employing advanced machine learning methodologies, the project aims to unearth the pivotal factors that significantly predispose individuals to poverty, thereby shedding light on the intricate web of influences that perpetuate economic disparity. The analytical journey encompasses preprocessing the data to ensure robustness, employing exploratory data analysis to unravel preliminary insights, and systematically applying feature selection techniques to distill the variables of highest relevance.

By crafting predictive models through a rigorous process of training, testing, and validation, this investigation aspires to provide a data-driven foundation for understanding poverty. The ultimate objective is to cultivate a rich body of knowledge that can underpin policy initiatives and intervention strategies. Through these insights, the study endeavors to contribute to the broader dialogue on poverty reduction, aiming to catalyze informed action that can significantly alter the socioeconomic landscape and improve the livelihoods of those at the margins of society in the United States.

## 2. Introduction

In an era where socio-economic disparities are increasingly under the spotlight, understanding the dynamics of poverty within the United States is more pertinent than ever. This report harnesses the data from the Household Survey USA 2024, a comprehensive dataset provided by the United States Census Bureau, to explore the multifaceted nature of poverty.

### 2.1 Context and Background

With the latest data sourced from [the United States Census Bureau](https://www.census.gov/programs-surveys/household-pulse-survey/data/datasets.2024.html#list-tab-1264157801), we delve into the current state of American households. By establishing a poverty threshold in accordance with the United Nations' guidelines, we aim to classify individuals as "poor" or "non-poor," thereby setting the stage for an in-depth analysis of poverty-related factors.

### 2.2 Motivation

As emerging data analysts, we are compelled by the capacity of data to unfold narratives of economic well-being and hardship. Through this project, we seek to quantify the correlates of poverty and, in doing so, contribute to the broader discourse on socio-economic health in the U.S.

### 2.3 Aim of the Investigation

Our primary objective is to discern the variables that most significantly influence the probability of an individual falling below the poverty line. The investigation will pivot on the following points of inquiry:

-   Identifying the demographic and socio-economic variables that heighten the risk of poverty.

-   Understanding the interaction between these variables and their collective impact on economic status.

### 2.4. Method & General Outlook

The methodology will involve rigorous data wrangling to prepare the dataset for analysis, followed by statistical techniques to evaluate the relationships between our variables of interest and poverty status.

The report will progress through the following structure:

-   **Section 2**: Articulates the contextual background and motivations behind the study.

-   **Section 3**: Provides an overview of the variables, describing each in detail and outlining the data wrangling process.

-   **Section 4**: Explores the analytical methods employed, drawing on relevant literature to situate our approach within the broader field of socio-economic analysis.

-   **Section 5**: Presents the findings of our study, addressing the initial research objectives and offering interpretations of the results.

-   **Section 6**: Discusses the implications of our findings from a socio-economic perspective, considering the potential for policy intervention and support.

-   **Section 7**: Lists all references and resources utilized throughout the research process.

## 3. Data Description

The core of our analysis is the Household Survey USA 2024 dataset. We will enrich this dataset by creating a new feature to categorize individuals based on their economic status, using the UN's poverty line as our benchmark. Our variables of interest are demographically and socio-economically diverse, encompassing race, age, gender, geographic location, education, marital status, and income.

### 3.1. Variable Description and Data-Set Preparation

The dataset utilized in this project, is titled "Household Survey USA 2024," originates from the official website of the United States Census Bureau. It constitutes a compilation of data collected through household surveys conducted across diverse regions of the United States. This dataset has the following variables:

```{r}

#|code-fold: true
#| code-summary: "show"
setwd("C:/Users/Hp/OneDrive/Desktop/Machine Learning")
data_set <- read.csv("data_set.csv")

prepared_dataset <- data_set %>%
  select(
    RecordID = SCRAM,
    HispanicOrigin = RHISPANIC, 
    Race = RRACE,  # 
    BirthYear = TBIRTH_YEAR,
    Gender = EGENID_BIRTH, 
    Location = REGION,  
    Education = EEDUC,  
    MaritalStatus = MS,  
    Income = INCOME,
    HouseholdSize = THHLD_NUMPER
  ) %>%
  mutate(
    Age = year(Sys.Date()) - BirthYear
  ) %>%
  select(-BirthYear)
```

#### 3.1.1 Variable Description and Dataset Preparation

In this step, we selected key variables like ID numbers, where people come from, their ethnicity, gender, education level, income, and age. The table gives a quick look at these details,including their names, data types, and brief explanations (prior the data transformation).

```{r}

variable_types <- as.data.frame(sapply(prepared_dataset, class))
variable_types <- cbind(Name = names(prepared_dataset), variable_types)
colnames(variable_types) <- c("Name", "Type")
rownames(variable_types) <- NULL

explanations <- c(
  "Identifier for each observation.",
  "Hispanic origin indicator where 1 means No, and 2 Yes.",
  "Race and ethnicity indicator,where 1 means White, 2 Black, 3 Asian, and 4 Other.",
  "Current gender identity, where 1 is Male, 2 Female, 3 Trasgender, and 4 Other",
  "Region code, where 1 means Northeast, 2 South, 3 Midwest, and 4 West.",
  "Education attainment, where 1 is Less than High School, 2 Some High School, 3 High School graduate, 4 College in progress, 5 AA or AS degree, 6 Bachelor's degree, and 7 Master's degree.",
  "Marital status, where 1 is Married, 2 Widowed, 3 Divorced, 4 Separated, and 5 Never married.",
  "Ordinal variable representing the household income before taxes  per year level, where 1 is Less than $25k, 2 is $25k-$34,9k, 3 is $35k-$49,9k, 4 is $50k-$74,9k, 5 is $75k-$99,9k, 6 is $100k-$149,9k, 7 is $150k-$199,9k. and 8 is higher than $200k.",
  "Total number of people in household. Numerical variable",
  "Variable representing the age"
)


variable_types$Explanation <- explanations


kable(variable_types)

```

### 3.3 Data Cleaning

Upon visualizing the variable types, it's evident that the majority are integers, signifying categorical data, while only 'Age' stands out as numerical. Additionally, 'RecordID' is characterized as a character variable, serving as a unique identifier.

```{r}


vis_dat(prepared_dataset, warn_large_data = FALSE) + 
  scale_fill_brewer(palette="Paired")


```

```{r}

prepared_dataset_modified <- prepared_dataset %>%
  mutate(across(everything(), ~ifelse(. == -99, NA, .)))

# Count NA values, including those originally set as -99
na_counts <- colSums(is.na(prepared_dataset_modified))

na_percentages <- (na_counts / nrow(prepared_dataset_modified)) * 100

# Create a data frame to display the counts
na_df <- data.frame(
  Variable = names(na_counts), 
  NAs = na_counts, 
  Percentage = sprintf("%.2f%%", na_percentages)  # formatted as percentage with 2 decimal places
)

kable(na_df, 
          options = list(
            dom = 'lBfrtip', 
            pageLength = 15,   
            lengthMenu = c(10, 15, 20), 
            paging = TRUE,
            searching = TRUE,
            ordering = TRUE
          ))
```

This table presents the count and percentage of missing values (NA) for each variable in the dataset. Notably, 'MaritalStatus' and 'Income' show some missing data, with 'Income' having the highest percentage of missing values at 1.84%. Additionally, it's important to note that in this dataset, the value -99 is interpreted as NA.

### 3.3 Data modifications

Eight histograms displaying the distributions variables within a data set will help us to determine which adjustments in the data we need to do.}.

```{r}
numeric_vars <- sapply(prepared_dataset, is.numeric)
numeric_cols <- names(numeric_vars[numeric_vars])
par(mfrow = c(ceiling(length(numeric_cols)/3), 2), mar = c(6, 6, 3, 2))
for (col in numeric_cols) {
  
  if (all(prepared_dataset[[col]] %in% c(1, 2), na.rm = TRUE)) {
    breaks_vals = c(0.5, 1.5, 2.5)  
  } else {
    if (all(prepared_dataset[[col]] %in% c(1, 2,3,4), na.rm = TRUE)) {
    breaks_vals = c(0.5, 1.5, 2.5, 3.5,4.5)  
  } else {
    range_vals = range(prepared_dataset[[col]], na.rm = TRUE)
    breaks_vals = seq(from = floor(range_vals[1]), to = ceiling(range_vals[2]), by = 1)
  }}

  hist(prepared_dataset[[col]], breaks = breaks_vals, main = paste("Distribution of", col), xlab = col,
       xaxt = 'n', right = FALSE)  

  axis(1, at = floor(min(breaks_vals)):ceiling(max(breaks_vals)), 
       labels = floor(min(breaks_vals)):ceiling(max(breaks_vals)))
}
```

In general, the change will involve adding the category 0 to all variables. Additionally, as part of this adjustment, categories with minority distributions will be grouped together. For example, in the 'Education' variable, the 7 categories will be reduced to just 5

#### Re-coding of Variables

The new categories for the following variables are as shown in the table below:

```{r}
variable_change <- cbind(c("RHispanic", "Gender", "Race", "Location", "Education", "Marital Status"), explanations)
colnames(variable_change) <- c("Name", "New categories")
rownames(variable_change) <- NULL

explanations <- c(
  "Where 0 means No, and 1 Yes.",
  "Where 0 means Male, 1 Female, and 3 Others.",
  "Where 0 means White, 1 Black, 2 Others.",
  "Where 0 means Northwest, 1 South, 2 Midwest, 3 West.",
  "Where 0 means No education, 1 High School graduate, 2 AA or AS degree, 3 Bachelor's degree, and 4 Master's degree.",
  "where 0 is Married, 1 Widowed, 2 Divorced, 3 Separated, and 4 Never married."
)



kable(variable_change)
```

#### NA Treatment

As the proportion of NAs is insignificant (0.6% in Marital Status and 1.9% in Income), these observations will be removed.

```{r}
#| code-fold: true
#| code-summary: "show"
prepared_dataset<-prepared_dataset%>%
  filter(
    MaritalStatus != -99,  # Exclude -99 from MaritalStatus
    Income != -99,  # Exclude -99 from Income
    Income != -88  # Exclude -88 from Income
  ) %>%
   mutate(
    HispanicOrigin = ifelse(HispanicOrigin == 1, 0, ifelse(HispanicOrigin == 2, 1, HispanicOrigin)),  # Recode HispanicOrigin
    Gender = case_when(
      Gender == 1 ~ 0, 
      Gender == 2 ~ 1,
      Gender == 3 ~ 2,
      TRUE ~ Gender
    ),  # Recode Gender
    Race = case_when(
      Race == 1 ~ 0,
      Race == 2 ~ 1,
      Race %in% c(3, 4) ~ 2,
      TRUE ~ Race
    ),  # Recode Race
    Location = case_when(
      Location == 1 ~ 0,
      Location == 2 ~ 1,
      Location == 3 ~ 2,
      Location == 4 ~ 3,
      TRUE ~ Location
    ),  # Recode Location
    Education = case_when(
      Education %in% c(1, 2) ~ 0,
      Education == 3 ~ 1,
      Education == 4 ~ 1,
      Education == 5 ~ 2,
      Education == 6 ~ 3,
      Education == 7 ~ 4,
      TRUE ~ Education
    ),  # Recode Education
    MaritalStatus = case_when(
      MaritalStatus == 1 ~ 0,
      MaritalStatus == 2 ~ 1,
      MaritalStatus == 3 ~ 2,
      MaritalStatus == 4 ~ 3,
      MaritalStatus == 5 ~ 4,
      TRUE ~ MaritalStatus
    ),  # Recode MaritalStatus
    HouseholdSize = case_when(
      HouseholdSize == 1 ~ 0,            
      HouseholdSize == 2 ~ 1,            
      HouseholdSize == 3 ~ 2,            
      HouseholdSize == 4 ~ 3,            
      HouseholdSize == 5 ~ 4,            
      HouseholdSize >= 6 ~ 5,            
      TRUE ~ HouseholdSize 
    ) 
  )

```

After all the changes we updated the histograms to showcase our dataset's new coding schemes, which significantly improve the clarity of our distribution visualizations.

```{r}
#| code-fold: true
#| code-summary: "show"
numeric_vars <- sapply(prepared_dataset, is.numeric)
numeric_cols <- names(numeric_vars[numeric_vars])
par(mfrow = c(ceiling(length(numeric_cols)/3), 2), mar = c(6, 6, 3, 2))
for (col in numeric_cols) {
  
  if (all(prepared_dataset[[col]] %in% c(0, 1), na.rm = TRUE)) {
    breaks_vals = c(-0.5,0.5, 1.5)  
  } else {
    if (all(prepared_dataset[[col]] %in% c(0,1, 2,3), na.rm = TRUE)) {
    breaks_vals = c(-0.5,0.5, 1.5, 2.5, 3.5)  
  } else {
    range_vals = range(prepared_dataset[[col]], na.rm = TRUE)
    breaks_vals = seq(from = floor(range_vals[1]), to = ceiling(range_vals[2]), by = 1)
  }}

  hist(prepared_dataset[[col]], breaks = breaks_vals, main = paste("Distribution of", col), xlab = col,
       xaxt = 'n', right = FALSE)  

  axis(1, at = floor(min(breaks_vals)):ceiling(max(breaks_vals)), 
       labels = floor(min(breaks_vals)):ceiling(max(breaks_vals)))
}
```

#### Distribution Transformation

```{r}
#| code-fold: true
#| code-summary: "show"
identify_outliers <- function(x) {
  q25 <- quantile(x, 0.25)
  q75 <- quantile(x, 0.75)
  iqr <- q75 - q25
  lower_bound <- q25 - 1.5 * iqr
  upper_bound <- q75 + 1.5 * iqr
  
  return(x < lower_bound | x > upper_bound)
}

# Apply function to identify outliers in the Age column
outliers <- identify_outliers(prepared_dataset$Age)

# View outliers
outliers_data <- prepared_dataset[outliers, ]
boxplot(prepared_dataset$Age)
```

The boxplot shows age distribution. In the graph, we can see that there are no outliers within the distribution of Age, as all the data points fall within the necessary quartiles. This suggests a consistent age range.

```{r}
#| code-fold: true
#| code-summary: "show"
set.seed(123)  
sampled_ages <- sample(prepared_dataset$Age, min(5000, length(prepared_dataset$Age)))
shapiro_test <- shapiro.test(sampled_ages)

# Print the results of the Shapiro-Wilk test
print(shapiro_test)
```

However, we performed the Shapiro-Wilk test on a sample of the 'Age' variable from our prepared dataset. Our test yielded a W statistic value of 0.97533 and a p-value significantly lower than 0.05, suggesting that the distribution of 'Age' is not normal. Then, we have to apply a logarithmic transformation to the 'Age' variable.

```{r}
#| code-fold: true
#| code-summary: "show"
prepared_dataset <- prepared_dataset %>%
  mutate(Log_Age = ifelse(Age > 0, log(Age), NA))  # Assign NA where Age is zero or negative to avoid errors

# Check the transformed distribution of Log_Age
ggplot(prepared_dataset, aes(x = Log_Age)) +
  geom_histogram(binwidth = 0.1, fill = "purple", alpha = 0.7) +
  ggtitle("Log Transformed Distribution of Age") +
  xlab("Log(Age)") +
  ylab("Count")
```

#### 3.1.4 Features creation

To answer the research question we need to create a binary variable (`PovertyStatus`) which will be 1 if the individual is living in Poverty according to the [Poverty Guidelines](https://aspe.hhs.gov/topics/poverty-economic-mobility/poverty-guidelines) of the Office of the Assistant Secretary For Planning and Evaluation of the USA.

```{r}
#| code-fold: true
#| code-summary: "show"
poverty_threshold <- c(15060, 20440, 25820, 31200, 36580, 41960, 47340, 52720)

prepared_dataset <- prepared_dataset %>%
  mutate(
    PovertyStatus = case_when(
      HouseholdSize == 1 & Income == 1 ~ 1, 
      HouseholdSize == 2 & Income %in% c(1) ~ 1, 
      HouseholdSize == 3 & Income %in% c(1) ~ 1, 
      HouseholdSize == 4 & Income %in% c(1, 2) ~ 1, 
      HouseholdSize == 5 & Income %in% c(1, 2) ~ 1, 
      HouseholdSize == 6 & Income %in% c(1, 2, 3) ~ 1, 
      HouseholdSize == 7 & Income %in% c(1, 2, 3) ~ 1, 
      HouseholdSize == 8 & Income %in% c(1, 2, 3) ~ 1, 
      TRUE ~ 0
    )
  )%>%
 mutate(
  PovertyStatus = factor(PovertyStatus, levels = c(0, 1), labels = c("Not Poor", "Poor"))
)
plot(prepared_dataset$PovertyStatus)
number_poor <- sum(prepared_dataset$PovertyStatus == 1)

# Calculating the total number of observations
total_observations <- nrow(prepared_dataset)

# Calculating the percentage of Poor people
percentage_poor <- (number_poor / total_observations) * 100

# Print the results
cat("Number of Poor People:", number_poor, "\n")
cat("Percentage of Poor People:", round(percentage_poor, 2), "%\n")
```

### 3.2 Exploratory data analysis

```{r}
result_html <- prepared_dataset %>%
  select(-RecordID) %>%
  summarytools::dfSummary() %>%
  summarytools::view(browser = TRUE)  # Esto abrirá el archivo HTML en tu navegador predeterminado
```

```{r}
variables <- c("HispanicOrigin", "Race", "Gender", "Location", "Education", "MaritalStatus", "Income", "HouseholdSize", "Log_Age")  
corr_s <- cor(prepared_dataset[, variables], method = "spearman")  

corr_df <- as.data.frame.table(corr_s)  

ggplot(corr_df, aes(x = Var1, y = Var2, fill = Freq)) +   geom_tile(color = "white") +   scale_fill_gradient2(low = "#0C2340FF", mid = "white", high = "#0F8D7BFF", midpoint = 0, limit = c(-1, 1)) +   theme_minimal() +   theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),         axis.text.y = element_text(size = 8),         plot.title = element_text(size = 14)) +   labs(title = "Spearman Correlation Matrix",        x = "Variables",        y = "Variables") +   scale_x_discrete(labels = c("Hispanic Origin", "Race", "Gender", "Location", "Education", "Marital Status", "Income", "Household Size", "Log(Age)")) +    scale_y_discrete(labels = c("Hispanic Origin", "Race", "Gender", "Location", "Education", "Marital Status", "Income", "Household Size", "Log(Age)")) 
```

```{r}
# Define the colors in the palette
my_palette <- c("#9EA2A2FF","#A1BAACFF","#C6D4D6FF","#78BE21FF","#0F8D7BFF",
                 "#5E81ACFF","#236192FF","#0C2340FF")

# Bar chart
ggplot(prepared_dataset, aes(x = factor(Race), fill = factor(Income))) +   geom_bar(position = "fill") +   facet_wrap(~ Gender) +   labs(title = "Income Distribution by Race and Gender",        x = "Race", y = "Revenue ratio") +   scale_fill_discrete(name = "Ingresos") +
  scale_fill_manual(values = my_palette)

```

```{r}

# Define custom labels for the Race variable
custom_labels <- c("White", "Black", "Others")

# Graph 1: Total income by race
plot1 <- ggplot(prepared_dataset, aes(x = factor(Race), fill = factor(Income))) +
  geom_bar(position = "stack") +
  labs(title = "Income Distribution by Race",
       x = "Race", y = "Sum of Income") +
  scale_fill_manual(values = my_palette, name = "Income") +
  scale_x_discrete(labels = custom_labels)

# Graph 2: Proportion of income by race
plot2 <- ggplot(prepared_dataset, aes(x = factor(Race), fill = factor(Income))) +
  geom_bar(position = "fill") +
  labs(title = "Income Distribution by Race",
       x = "Race", y = "Proportion of Income") +
  scale_fill_manual(values = my_palette, name = "Income") +
  scale_x_discrete(labels = custom_labels)

# graphics side by side
grid.arrange(plot1, plot2, ncol = 2)
```

```{r}
ggplot(prepared_dataset, aes(x = Age, fill = factor(Income))) +
  geom_bar(position = "stack") +
  labs(title = "Income Distribution by Age",
       x = "Age", y = "Sum of Income") +
  scale_fill_manual(values = my_palette, name = "Income")
```

```{r}
# Define the color palette
my_palette <- c("#9EA2A2FF","#A1BAACFF","#C6D4D6FF","#78BE21FF","#0F8D7BFF",                  "#5E81ACFF","#236192FF","#0C2340FF")  
# Stacked bar chart of Education by Income with summation 
ggplot(prepared_dataset, aes(x = as.factor(Education), fill = factor(Income))) +   geom_bar() +  
  # Stacked bars    
  labs(title = "Education Distribution by Income Interval",        x = "Education Level", y = "Sum of Education Level") +   scale_fill_manual(values = my_palette, name = "Income Interval") +  
  
  theme_minimal() +  
  
  scale_x_discrete(labels = c("Less than High School", "High School", "AA degree", "Bachelor's degree", "Master's degree")) +  
  # assigning x axis labels to text   
  theme(axis.text.x = element_text(size = 8), axis.text.y = element_text(size = 8))  # 
```

## 4.Methods

This section outlines the methodology employed to determine the most effective predictive model for analyzing poverty trends in the United States. The dataset includes a comprehensive range of socio-economic indicators, demographic attributes, and other relevant factors collected through household surveys conducted across various regions of the country.The target variable for prediction is the poverty status of individuals or households, with various supervised and unsupervised learning techniques explored to identify predictive patterns. Utilizing the R programming language, the analysis encompasses both Exploratory Data Analysis (EDA) and model development stages.

Regression algorithms constitute a primary focus due to the goal of accurately predicting poverty status based on relevant attributes. The dataset is split into training and testing sets using a ratio of 70:30, ensuring sufficient data for model training and evaluation. Specifically, 70% of the data is allocated for training the models, while 15% is reserved for validation purposes. The remaining 15% serves as the testing set to assess model performance on unseen data, ensuring the reliability and generalizability of the predictive models.

## 4.1 Unsupervised Methods

### 4.1.1 Clustering

### 4.2 Supervised Methods

```{r}
#| code-fold: true
#| code-summary: "show"
#Create training sets
set.seed(145)
index <- sample(1:nrow(prepared_dataset))
n_tr <- round(0.70 * nrow(prepared_dataset))
n_val <- round(0.15 * nrow(prepared_dataset))

ds_tr <- prepared_dataset[index[1:n_tr], ]  # First 70% for training
ds_val <- prepared_dataset[index[(n_tr + 1):(n_tr + n_val)], ]  # Next 15% for validation
ds_te <- prepared_dataset[index[(n_tr + n_val + 1):nrow(prepared_dataset)], ] #Remaining for testing

ds_tr<-ds_tr[-c(1,8,10)]
ds_te<-ds_te[-c(1,8,10)]
ds_val<-ds_val[-c(1,8,10)]

#downsampling
set.seed(123) ## for reproducibility
ds_tr_down <- downSample(subset(ds_tr, select=-PovertyStatus), y=ds_tr$PovertyStatus, yname="PovertyStatus")


```

#### *4.2.2 Logistic Regression*

As a first model we built a Logistic regression model, initially using all available features related to poverty status. Employing backward stepwise selection based on AIC, we refined the model, retaining significant variables such as Hispanic origin, race, gender, education, marital status, household size, and age. Evaluating its performance, we assessed metrics like accuracy, sensitivity, specificity, precision, F1 score, and AUC. To address class imbalance, we implemented downsampling of the majority class, improving the model's generalization across both classes.

```{r}
#| code-fold: true
#| code-summary: "show"
library(MASS)
initial_model <- glm(PovertyStatus ~ ., family = binomial, data = ds_tr)
selected_model <- stepAIC(initial_model, direction = "backward")
summary(selected_model)
```

```{r}
#| code-fold: true
#| code-summary: "show"
library(glmnet)
library(pROC)
x <- as.matrix(ds_tr[, c("MaritalStatus", "HouseholdSize", "Education", "Log_Age","Race", "HispanicOrigin", "Gender", "Location" )])
y <- ds_tr$PovertyStatus

cv_model <- cv.glmnet(x, y, family = "binomial", alpha = 1, type.measure = "class")
optimal_lambda <- cv_model$lambda.min
final_model <- glmnet(x, y, family = "binomial", alpha = 1, lambda = optimal_lambda)

predicted_prob_train <- predict(final_model, newx = x, type = "response")
predicted_classes_train <- ifelse(predicted_prob_train > 0.5, 1, 0)
train_conf_matrix <- table(Predicted = predicted_classes_train, Actual = y)
roc_curve_train <- roc(y, predicted_prob_train)
auc_train <- auc(roc_curve_train)

print(train_conf_matrix)
print(paste("AUC for Training Set:", auc_train))

```

```{r}
library(caret)
library(pROC)
set.seed(123) 

model <- glm(PovertyStatus ~ HispanicOrigin + Race + Gender + 
    Education + MaritalStatus + HouseholdSize + Log_Age + Location , family = binomial, data = ds_tr)
summary(model)

# Training set evaluation
predicted_prob_train <- predict(model, newdata = ds_tr, type = "response")
roc_curve_train <- roc(ds_tr$PovertyStatus, predicted_prob_train)
auc_train <- auc(roc_curve_train)

# Validationing set evaluation
predicted_prob_val <- predict(model, newdata = ds_val, type = "response")
roc_curve_val <- roc(ds_val$PovertyStatus, predicted_prob_val)
auc_val <- auc(roc_curve_val)

# Testing set evaluation
predicted_prob_test <- predict(model, newdata = ds_te, type = "response")
roc_curve_test <- roc(ds_te$PovertyStatus, predicted_prob_test)
auc_test <- auc(roc_curve_test)

# Computing performance metrics
conf_matrix_train <- table(Predicted = ifelse(predicted_prob_train > 0.5, "Poor", "Not Poor"), Actual = ds_tr$PovertyStatus)
conf_matrix_val <- table(Predicted = ifelse(predicted_prob_val > 0.5, "Poor", "Not Poor"), Actual = ds_val$PovertyStatus)
conf_matrix_test <- table(Predicted = ifelse(predicted_prob_test > 0.5, "Poor", "Not Poor"), Actual = ds_te$PovertyStatus)

accuracy_train <- sum(diag(conf_matrix_train)) / sum(conf_matrix_train)
accuracy_val <- sum(diag(conf_matrix_val)) / sum(conf_matrix_val)
accuracy_test <- sum(diag(conf_matrix_test)) / sum(conf_matrix_test)

sensitivity_train <- conf_matrix_train[2, 2] / sum(conf_matrix_train[2, ])
specificity_train <- conf_matrix_train[1, 1] / sum(conf_matrix_train[1, ])
precision_train <- conf_matrix_train[2, 2] / sum(conf_matrix_train[, 2])
f1_score_train <- 2 * precision_train * sensitivity_train / (precision_train + sensitivity_train)

sensitivity_val <- conf_matrix_val[2, 2] / sum(conf_matrix_val[2, ])
specificity_val <- conf_matrix_val[1, 1] / sum(conf_matrix_val[1, ])
precision_val <- conf_matrix_val[2, 2] / sum(conf_matrix_val[, 2])
f1_score_val <- 2 * precision_val * sensitivity_val / (precision_val + sensitivity_val)

sensitivity_test <- conf_matrix_test[2, 2] / sum(conf_matrix_test[2, ])
specificity_test <- conf_matrix_test[1, 1] / sum(conf_matrix_test[1, ])
precision_test <- conf_matrix_test[2, 2] / sum(conf_matrix_test[, 2])
f1_score_test <- 2 * precision_test * sensitivity_test / (precision_test + sensitivity_test)

# Data frame with metrics
metrics <- data.frame(
  Set = c("Training", "Validation", "Testing"),
  Accuracy = c(accuracy_train, accuracy_val, accuracy_test),
  Sensitivity = c(sensitivity_train, sensitivity_val, sensitivity_test),
  Specificity = c(specificity_train, specificity_val, specificity_test),
  Precision = c(precision_train, precision_val, precision_test),
  F1_Score = c(f1_score_train, f1_score_val, f1_score_test),
  AUC = c(auc_train, auc_val, auc_test)
)

knitr::kable(metrics, caption = "Model Evaluation Metrics")

# ROC curves
plot(roc_curve_train, main = "ROC Curve for Training Data", col = "blue")
abline(a = 0, b = 1, lty = 2, col = "gray")
plot(roc_curve_val, add = TRUE, col = "green")
plot(roc_curve_test, add = TRUE, col = "red")
legend("bottomright", legend = c("Training", "Validation", "Test"), col = c("blue", "green", "red"), lwd = 2)


```

```{r}

library(dplyr)
library(knitr)
model <- glm(PovertyStatus ~ HispanicOrigin + Race + Gender + 
             Education + MaritalStatus + HouseholdSize + Log_Age + Location,
             family = binomial, data = ds_tr_down)

# Function to calculate metrics, including balanced accuracy
calculate_metrics <- function(data, model) {
  predicted_prob <- predict(model, newdata = data, type = "response")
  predicted_classes <- ifelse(predicted_prob > 0.5, "Poor", "Not Poor")
  conf_matrix <- table(Predicted = predicted_classes, Actual = data$PovertyStatus)
  
  accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
  sensitivity <- conf_matrix["Poor", "Poor"] / sum(conf_matrix[, "Poor"])
  specificity <- conf_matrix["Not Poor", "Not Poor"] / sum(conf_matrix[, "Not Poor"])
  precision <- conf_matrix["Poor", "Poor"] / sum(conf_matrix["Poor", ])
  f1_score <- 2 * precision * sensitivity / (precision + sensitivity)
  balanced_accuracy <- (sensitivity + specificity) / 2
  
  return(data.frame(
    Accuracy = accuracy,
    Sensitivity = sensitivity,
    Specificity = specificity,
    Precision = precision,
    F1_Score = f1_score,
    Balanced_Accuracy = balanced_accuracy
  ))
}

# Computing metrics for all sets
metrics_train <- calculate_metrics(ds_tr, model)
metrics_val <- calculate_metrics(ds_val, model)
metrics_test <- calculate_metrics(ds_te, model)

metrics_train$Set <- "Training"
metrics_val$Set <- "Validation"
metrics_test$Set <- "Testing"

# Combining and displaying metrics for all sets
metrics_combined <- rbind(metrics_train, metrics_val, metrics_test)
knitr::kable(metrics_combined, caption = "Model Evaluation Metrics for Training, Validation, and Testing Sets")

```

The logistic regression analysis reveals significant insights into the factors influencing poverty status among individuals in the dataset. **Education** emerges as the most critical determinant, where each incremental increase in educational attainment significantly decreases the likelihood of being categorized as poor, underscoring education as a vital lever in socio-economic mobility. **Household Size** is also a significant predictor, with larger household sizes increasing the probability of poverty, reflecting the economic pressures of supporting more individuals. **Marital Status** shows that married individuals are less likely to be poor, suggesting the economic benefits of shared financial responsibilities and resources. Notably, **Race** and **Hispanic Origin** also play roles in influencing poverty likelihood, indicating existing disparities that could be targets for social policies. The model achieves a **Balanced Accuracy** of approximately 74% across training, validation, and testing sets, demonstrating consistent performance and good generalization of the model. This balanced accuracy, closely mirroring the overall accuracy, suggests that the model is equally adept at identifying both poor and not poor individuals, which is critical in ensuring fairness in predictive assessments used for social interventions.

#### 4.2.3 CART (Decision Tree)

The second model we tested is the Decision Tree. Initially, we began with an extensive tree and identified a need to address an imbalance issue in the training set. To resolve this problem, we applied downsampling. After adjusting the dataset, we proceeded to prune the tree applying the 1-SE rule to reduce the chances of overfitting our data.

The resultant tree is the following:

```{r}

#Creating the extensive tree
poverty_ct <- rpart(PovertyStatus ~ ., data=ds_tr,control = list(cp=0.001) )
rpart.plot(poverty_ct)

#Confusio matrix extensive tree
caret::confusionMatrix(reference=ds_tr$PovertyStatus, data=predict(poverty_ct,type = "class"),
                       positive="Poor")
caret::confusionMatrix(reference=ds_val$PovertyStatus, data=predict(poverty_ct, newdata=ds_val, type="class"),
                       positive="Poor")

#downsampling
#set.seed(123) ## for reproducibility
#ds_tr_down <- downSample(subset(ds_tr, select=-PovertyStatus), y=ds_tr$PovertyStatus, yname="PovertyStatus")

#Facing imbalance issue
poverty_ct <- rpart(PovertyStatus ~ ., data=ds_tr_down, control = list(cp=0.001))
rpart.plot(poverty_ct)

#Confusio matrix after imbalance issue
caret::confusionMatrix(reference=ds_tr_down$PovertyStatus, data=predict(poverty_ct,type = "class"),
                       positive="Poor")
caret::confusionMatrix(reference=ds_val$PovertyStatus, data=predict(poverty_ct, newdata=ds_val, type="class"),
                       positive="Poor")
#Pruning
set.seed(123) ## for reproducibility
plotcp(poverty_ct, upper = "split") # -> select leftmost mean under the line 
printcp(poverty_ct) # look for the corresponding cp
poverty_ct_prun <- prune(poverty_ct, cp= 0.0050000) # 10 splits, 10 0.0050000

#Confusio matrix pruned tree
caret::confusionMatrix(reference=ds_tr_down$PovertyStatus, data=predict(poverty_ct_prun,type = "class"),
                       positive="Poor")
caret::confusionMatrix(reference=ds_val$PovertyStatus, data=predict(poverty_ct_prun, newdata=ds_val, type="class"),
                       positive="Poor") 


```

```{r}
rpart.plot(poverty_ct_prun)
```

```{r}
cm <- confusionMatrix(reference=ds_te$PovertyStatus, 
                      data=predict(poverty_ct_prun, newdata=ds_te, type="class"),
                      positive="Poor")

# Extract metrics
accuracy_tree <- cm$overall['Accuracy']
balanced_accuracy_tree <- cm$byClass['Balanced Accuracy']
sensitivity_tree <- cm$byClass['Sensitivity']
specificity_tree <- cm$byClass['Specificity']

# Create a data table for plotting

metrics_table <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "Sensitivity", "Specificity"),
  Value = as.numeric(c(accuracy_tree, balanced_accuracy_tree, sensitivity_tree, specificity_tree))
)

# Format the Value column to display only three decimal places
metrics_table$Value <- round(metrics_table$Value, 3)

# Use kable to create a nicely formatted table
kable(metrics_table, format = "html", digits = 3, caption = "Classification Metrics CART")
```

The results of our decision tree are promising. Not only does it avoid overfitting and imbalance issues, but it also achieves both high overall and balanced accuracy, both nearing 80%. The similarity between these two metrics allows us to assert that its classification power is decent.

Regarding the interpretation of the model, we can say that the education level of individuals in the United States is the most significant variable in determining whether they will live in poverty. Holding at least a bachelor's degree significantly reduces the likelihood of an individual being poor. The second most important variable is the number of family members, and we observe that households with only one person tend to avoid poverty. As the third most significant variable, marital status shows that being married decreases the likelihood of living in poverty.

Interestingly, race ranks as the fifth most important variable. This indicates that not being white in someway increases the chance of being poor within American society.

#### 4.2.4 Random Forest

Since we achieved good results with the previously mentioned tree, we wanted to explore an option that could further reduce the likelihood of overfitting our data. Therefore, we proceed to analyze the random forest method and conducted a hyperparameter tuning using a limited number of iterations due to computational capacity and waiting time for results. The goal with this model is to achieve higher accuracy and reduce the probability of overfitting, as we loose the interpretability.

```{r}
#Reviewing levels
levels(ds_tr_down$PovertyStatus)
levels(ds_val$PovertyStatus)
levels(ds_te$PovertyStatus)
all_levels <- union(levels(ds_tr_down$PovertyStatus), levels(ds_val$PovertyStatus))
all_levels <- union(all_levels, levels(ds_te$PovertyStatus))

ds_tr_down$PovertyStatus <- factor(ds_tr_down$PovertyStatus, levels = all_levels)
ds_val$PovertyStatus <- factor(ds_val$PovertyStatus, levels = all_levels)
ds_te$PovertyStatus <- factor(ds_te$PovertyStatus, levels = all_levels)

##Tuning
# Define the task
task <- makeClassifTask(data = ds_tr_down, target = "PovertyStatus")

# Define the learner with potentially simpler settings
learner <- makeLearner("classif.ranger", predict.type = "prob", par.vals = list(min.node.size = 10))

# Define a focused parameter set
num_predictors <- ncol(ds_tr_down) - 1
ps_focused <- makeParamSet(
  makeIntegerParam("mtry", lower = 2, upper = min(4, num_predictors))  # Even narrower range
)

# Define control for random search with fewer iterations
ctrl <- makeTuneControlRandom(maxit = 20)  # Even fewer iterations

# Tune the model with fewer CV folds to save time
res <- tuneParams(learner, task, resampling = makeResampleDesc("CV", iters = 3), par.set = ps_focused, control = ctrl)

##Tuning end

#Applying the tuned hyperparameters


optimal_model <- setHyperPars(learner, par.vals = res$x)
optimal_model <- mlr::train(optimal_model, task)

#Metrics

pred_rf_tr <- predict(optimal_model, newdata = ds_tr_down, type="response")
pred_rf_val <- predict(optimal_model, newdata = ds_val, type="response")
pred_rf_te <- predict(optimal_model, newdata = ds_te, type="response")

#levels_all <- union(levels(ds_tr_down$PovertyStatus), levels(pred_rf_tr$data))

#pred_rf_tr$data <- factor(pred_rf_tr$data, levels = levels_all)
#ds_tr_down$PovertyStatus <- factor(ds_tr_down$PovertyStatus, levels = levels_all)
#all_levels <- unique(c(levels(ds_tr_down$PovertyStatus), levels(pred_rf_tr$data)))

#ds_tr_down$PovertyStatus <- factor(ds_tr_down$PovertyStatus, levels = all_levels)
#pred_rf_tr$data <- factor(pred_rf_tr$data, levels = all_levels)


train_perf_rf <- performance(pred_rf_tr , measures = list(acc, bac, fpr, tpr))

val_perf_rf <- performance(pred_rf_val, measures = list(acc, bac, fpr, tpr))

test_perf_rf <- performance(pred_rf_te, measures = list(acc, bac, fpr, tpr))

train_specificity_rf <- 1 - performance(pred_rf_tr, measures = fpr)
train_sensitivity_rf <- performance(pred_rf_tr, measures = tpr)

val_specificity_rf <- 1 - performance(pred_rf_val, measures = fpr)
val_sensitivity_rf <- performance(pred_rf_val, measures = tpr)

test_specificity_rf <- 1 - performance(pred_rf_te, measures = fpr)
test_sensitivity_rf <- performance(pred_rf_te, measures = tpr)

```

```{r}
accuracy_rf <- test_perf_rf[1]
balanced_accuracy_rf <- test_perf_rf[2]
sensitivity_rf <- test_sensitivity_rf
specificity_rf <- test_specificity_rf



metrics_table_rf <- data.frame(
  Metric = c("Accuracy", "Balanced Accuracy", "Sensitivity", "Specificity"),
  Value = as.numeric(c(accuracy_rf, balanced_accuracy_rf, sensitivity_rf, specificity_rf))
)

# Format the Value column to display only three decimal places
metrics_table_rf$Value <- round(metrics_table_rf$Value, 5)

# Use kable to create a nicely formatted table
kable(metrics_table_rf, format = "html", digits = 5, caption = "Classification Metrics Random Forest")
```

Once the final numbers are in, it becomes clear that the accuracy of the model did not improve compared to a normal, simple tree. Given the increased complexity and the lack of interpretability of this method, the random forest does not emerge as the preferred choice.

#### 4.2.5 Support Vector Machine

##### a. Linear Support Vector Machine:

First a Linear support vector Machine model was created, and then a prediction was performed with the model using the validation set, in order to evaluate it with the confussion Matrix. Then the model was trained with Cross Validation to confirm that the model is able to generalize new data well, and the validation accuracy is high (≈74%). Next, the model was fine-tuned to maximize its performance, so that it is not over-fitted. To do this, the model was trained with various values of C and thus select the parameter that can improve the model's ability to generalize data. Finally, after the result of the confusion matrix of the tuned model with a new C , the accuracy has almost keep the same, without any significant improvement.

```{r}
inclinear_svm <- svm(PovertyStatus ~ ., data=ds_tr_down, kernel="linear")
inclinear_svm
inclinear_svm_pred <- predict(inclinear_svm, newdata = ds_val)

table(Pred=inclinear_svm_pred, obs=ds_val$PovertyStatus)
confusionMatrix(data=inclinear_svm_pred, reference = ds_val$PovertyStatus )
```

```{r}
trctrl <- caret::trainControl(method = "cv", number = 10)
set.seed(143)
inclinear_svmt <- caret::train(PovertyStatus ~ ., data = ds_tr_down, method = "svmLinear",
                               trControl = trctrl)
print(inclinear_svmt)

```

```{r}
grid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))
grid
set.seed(143)
inclinear_svmtgrid <- caret::train(PovertyStatus ~ ., data = ds_tr_down, method = "svmLinear",
                           trControl=trctrl,
                           tuneGrid = grid)
inclinear_svmtgrid
plot(inclinear_svmtgrid)
```

```{r}
# Train the SVM model with a linear kernel and a cost of 0.1
inclineart_svm <- svm(PovertyStatus ~ ., data = ds_tr_down, kernel = "linear", cost = 0.1)
print(inclinear_svm)  # Print the model summary to check the configuration

# Make predictions on the validation dataset
inclineart_svm_pred <- predict(inclineart_svm, newdata = ds_val)

# Create a contingency table with predictions and observed values
# NOTE: Ensure that the dataset used for observed values matches the prediction dataset
table(Pred = inclineart_svm_pred, obs = ds_val$PovertyStatus)

# Calculate and display the confusion matrix using the test dataset
# Ensure consistency in datasets used for predictions and reference
confusionMatrix(data = inclineart_svm_pred, reference = ds_val$PovertyStatus)
```

```{r}
# Assume you have stored the confusion matrix result in a variable called result_matrix
matrixsvmlint <- confusionMatrix(data=inclineart_svm_pred, reference = ds_val$PovertyStatus)

# Create a dataframe with the required values
summary_svmlint <- data.frame(
  Sensitivity = matrixsvmlint$byClass['Sensitivity'],
  Specificity = matrixsvmlint$byClass['Specificity'],
  Kappa = matrixsvmlint$overall['Kappa'],
  Accuracy = matrixsvmlint$overall['Accuracy'],
  Balanced_Accuracy = matrixsvmlint$byClass['Balanced Accuracy']
)

# Display the table
print(summary_svmlint)
```

##### b. Kernel Radial (Gaussiano):

To continue, a new Support Vector Machine model was created with a radial kernel, and then a prediction was performed with the model using the validation set. It is observed that both the Accuracy and the Balanced Accuracy of this new model are better than the linear SVM model, although only by a few points. Then, the model was re-trained with the entire training set using optimal hyperparameters for the radial basis kernel.

```{r}
incradial_svm <- svm(PovertyStatus ~ ., data = ds_tr_down, kernel = "radial")
print(incradial_svm)  
incradial_svm_pred <- predict(incradial_svm, newdata = ds_val)
confusionMatrix(data=incradial_svm_pred, reference = ds_te$PovertyStatus )
```

```{r}
grid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),
                           C = c(1, 10, 100, 500, 1000))
grid_radial
set.seed(143)
incradial_svmt <- caret::train(PovertyStatus ~ ., data = ds_tr_down, method = "svmRadial",
                           trControl=trctrl,
                           tuneGrid = grid_radial)
incradial_svmt
plot(incradial_svmt)
incradial_svmt$bestTune
```

```{r}
incradial_svmtuned  <- svm(PovertyStatus ~ ., data = ds_tr_down,
                         kernel = "radial", gamma = incradial_svmt$bestTune$sigma,
                         cost = incradial_svmt$bestTune$C)
incradial_svmtuned_pred <- predict(incradial_svmtuned, newdata = ds_val)
confusionMatrix(data=incradial_svmtuned_pred, reference = ds_val$PovertyStatus)
```

```{r}
# Assume you have stored the confusion matrix result in a variable called result_matrix
matrix_svmrad <- confusionMatrix(data=incradial_svmtuned_pred, reference = ds_val$PovertyStatus)

# Create a dataframe with the required values
summary_svmrad <- data.frame(
  Sensitivity = matrix_svmrad$byClass['Sensitivity'],
  Specificity = matrix_svmrad$byClass['Specificity'],
  Accuracy = matrix_svmrad$overall['Accuracy'],
  Kappa = matrix_svmrad$overall['Kappa'],
  Balanced_Accuracy = matrix_svmrad$byClass['Balanced Accuracy']
)

# Display the table
print(summary_svmrad)
```

Finally, after the result of the confussion matrix of the tunned model with a new Gamma and C , the accuracy has increased a really little amount , but at least it has increased. As a result, the model Support Vector Machine is not to much interpretable, we can just use the model if after comparing with the other models it has a better accuracy and balanced accuracy.

## 5. Results

```{r}
# Load necessary library
library(knitr)

# Create a data frame with Balanced Accuracy for each model
model_comparisons <- data.frame(
  Model = c("Logistic Regression", "CART (Decision Tree)", "Random Forest", "Linear SVM", "Kernel Radial SVM"),
  Balanced_Accuracy = c(0.73, 0.78, 0.78, 0.73, 0.78),  
  Training_Time = c("Fast", "Fast", "Moderate", "Slow", "Slow")  
)

# Create a table using knitr::kable
kable(model_comparisons, format = "markdown", caption = "Comparison of Balanced Accuracy and Training Time Across Models", align = c('l', 'c', 'c'))


```

When comparing five well defined predictive models for determining poverty status, Logistic Regression, CART (Decision Tree), Random Forest, Linear SVM, and Kernel Radial (Gaussian SVM), each model presents unique strengths and weaknesses in terms of balanced accuracy and computational efficiency. The CART model excels with the highest balanced accuracy, nearing 80%, indicating it effectively manages the class imbalance problem and provides reliable predictions across both poor and not poor categories. This model's simplicity also allows for quicker training and prediction times compared to more complex models, making it highly suitable for operational environments where time and resource efficiency are important.

The Random Forest model, while utilizing the robustness of multiple trees, did not show a significant improvement in balanced accuracy over the single CART model and required substantially more computational resources. This suggests that the additional complexity of the Random Forest does not translate into proportional gains in this specific scenario.

On the other hand, the SVM models, both linear and with a Gaussian kernel, although they provide reasonable balanced accuracy, fall short of the Decision Tree models. The Gaussian SVM marginally outperforms the linear SVM in terms of accuracy but at the cost of significantly increased computation time, which might not be justifiable in many practical applications.

Logistic Regression, being the simplest among the models, offers the quickest training times but has the lowest balanced accuracy, which might limit its usefulness in scenarios where accurate class prediction is crucial, especially under class imbalance conditions.

## 6. Recommendations and discussion

**6.1. Summary of Findings**

Our comparative analysis of five predictive models ,Logistic Regression, CART (Decision Tree), Random Forest, Linear SVM, and Kernel Radial SVM, revealed that the CART model provided the highest balanced accuracy, nearly 80%. This model also demonstrated a favorable balance between computational demand and predictive accuracy. The models' effectiveness varied, with the Random Forest and SVM models offering competitive but slightly lower performance metrics and requiring greater computational resources.

**6.2. Practical Implications**

-   **Policy Development**: The models have consistently identified education as a critical determinant of poverty status. Policymakers could leverage these insights to design educational programs targeted at vulnerable populations to reduce poverty rates. Additionally, interventions to support larger households and improve marital stability could also be beneficial, as indicated by the models.

-   **Resource Allocation**: By utilizing the predictive power of these models, government and non-profit organizations can better allocate resources, such as social welfare benefits and educational grants, to those most likely to be in poverty.

-   **Tool Deployment**: Integrating the CART model into social service databases can help continuously monitor at-risk populations and dynamically adjust interventions as circumstances change.

**6.3. Model Recommendations**

Based on our findings, we recommend the CART (Decision Tree) model for immediate deployment in poverty prediction tasks due to its high accuracy and efficiency. This model's ability to handle class imbalances and provide interpretable results makes it particularly useful for stakeholders needing clear and actionable insights.

**6.4. Limitations**

-   **Data Limitations**: The models were developed based on available data, which may not capture all nuances of poverty, such as transient poverty or underreported minority groups.

-   **Model Constraints**: While CART provides a good balance, it may still suffer from overfitting if not properly pruned. Random Forest and SVM models, while robust, require significant computational resources that might not be available in all settings.

-   **Computational Constraints**: The more complex models, particularly the Kernel Radial SVM, demand substantial computational power, which could be a limiting factor in low-resource settings.

**6.5. Future Research Directions**

-   **Incorporating Additional Data**: Future studies could explore the inclusion of more dynamic variables, such as employment trends or economic indicators, to enhance the models' sensitivity to economic shifts.

-   **Advanced Modeling Techniques**: Investigating deep learning approaches or ensemble methods that combine the strengths of multiple models could offer improvements in predictive accuracy.

-   **Cross-Validation Studies**: Extensive cross-validation across different geographic and socio-economic contexts would help validate the models' applicability and robustness universally.

**6.6. Conclusion**

The use of advanced predictive modeling in poverty assessment has significant potential to transform social welfare strategies. By accurately identifying those at greatest risk, resources can be more effectively directed, and interventions can be timely and relevant. The CART model, in particular, stands out as an efficient tool for such initiatives, promising substantial impacts on poverty mitigation efforts.

## 7. References
